{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Sprint_Challenge_Taxi.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "t2COwExFCNg4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning Sprint Challenge - play Taxi**\n",
        "For the sprint challenge, we will apply the techniques we have learned to play Taxi, an environment in the OpenAI Gym. In this task the agent controls a taxi that can navigate between four locations. The goal is to pick up a passenger from one location and drop them off to another. You get 20 points for each successful drop off, but lose 1 point for each step you take, and additionally there is a 10 point penalty for illegal pick-up/drop-off actions.\n",
        "\n",
        "You can create the environment and watch a random agent play with this code:"
      ]
    },
    {
      "metadata": {
        "id": "tbSoc3irCEW8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "024bcc19-777f-4e3f-d2bd-9c2d5b406b12"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.5)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.4.16)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CUtjiju2VE3k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import gym\n",
        "from gym import wrappers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcMPKJlQB6_W",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 20959
        },
        "outputId": "c43acdfb-72a7-4eb6-962b-4384301c0219"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v2')\n",
        "state = env.reset()\n",
        "env.render()\n",
        "print(env.step(action))\n",
        "\n",
        "total_reward = 0\n",
        "done = False\n",
        "while not done:\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "    print(env.step(action))\n",
        "    print('Cumulative reward:', total_reward, '\\n')\n",
        "\n",
        "print('Total reward:', total_reward)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "(27, -10, False, {'prob': 1.0})\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(27, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -10 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -11 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -21 \n",
            "\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(27, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -22 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -23 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -33 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -43 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -44 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -45 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -46 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -47 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -48 \n",
            "\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(7, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -58 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(107, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -59 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(107, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -60 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(207, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -61 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(207, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -71 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(207, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -72 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(227, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -73 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(247, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -74 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(247, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -84 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(227, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -85 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(247, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -86 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(247, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -96 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(247, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -106 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(347, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -107 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(347, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -117 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(447, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -118 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(447, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -128 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(447, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -138 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -139 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -149 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -159 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(327, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -160 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(327, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -161 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(327, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -171 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(327, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -181 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -182 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -183 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -184 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -185 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -195 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -196 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -206 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -207 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(427, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -217 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(447, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -218 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(447, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -219 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(447, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -229 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(447, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -239 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(447, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -249 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(447, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -259 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(347, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -260 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(347, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -270 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(347, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -280 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(347, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -290 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(247, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -291 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(247, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -301 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(147, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -302 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(147, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -312 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(147, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -322 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(127, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -323 \n",
            "\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(27, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -324 \n",
            "\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(27, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -325 \n",
            "\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(27, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -326 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(127, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -327 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(127, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -337 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(147, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -338 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(167, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -339 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(167, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -349 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -350 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(167, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -351 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(187, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -352 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(167, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -353 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -354 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(287, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -355 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(387, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -356 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(387, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -366 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(387, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -367 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(387, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -377 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(287, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -378 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(287, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -379 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -380 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -390 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(367, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -391 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(367, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -401 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(387, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -402 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(367, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -403 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -404 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -414 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -424 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "(287, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -425 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "(387, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -426 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(367, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -427 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -428 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -438 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -448 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "(267, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -458 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "(167, -10, False, {'prob': 1.0})\n",
            "Cumulative reward: -459 \n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "(147, -10, True, {'prob': 1.0})\n",
            "Cumulative reward: -460 \n",
            "\n",
            "Total reward: -460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gUiLA-l9ClAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You'll see that a random agent doesn't do very well - in a trial run the score reached -713 before the environment terminated."
      ]
    },
    {
      "metadata": {
        "id": "fiQaKwdRCncZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Instructions**\n",
        "Make a Python notebook where you work on the below goals. You can use whatever environment you wish to develop, but for turning in you should add the file to the ML-Reinforcement-Learning repository in the sprintchallenge/ directory. Add, commit, push, and it will appear in your already open pull request.\n",
        "\n",
        "The goals involve trying to beat a score in Taxi - be sure to measure the score of your approach after it is trained, and not during the training. This snippet measures performance (run a simulation repeatedly and average total rewards):"
      ]
    },
    {
      "metadata": {
        "id": "7byW93KjCmLG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5f498b05-f99a-445f-e50a-d12f69943e7d"
      },
      "cell_type": "code",
      "source": [
        "for episode in range(episodes):\n",
        "    state = env.reset()  # Assuming you already have env created as above\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        action = env.action_space.sample()  # TODO your policy here!\n",
        "        state, reward, done, info = env.step(env.action_space.sample())\n",
        "        total_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "    rewards.append(total_rewards)        \n",
        "\n",
        "print('Average score over time:', sum(rewards) / episodes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average score over time: -387.148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VrzQKMRJC1--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Goal 1 - Beat Random**\n",
        "As an initial goal, come up with an agent/policy that does better than random. And more specifically, try to at least have a positive score (>0) average.\n",
        "\n",
        "This game is discrete, and so you can use the Q-learning approach and build a matrix of states by actions populated with expected rewards. This approach should work well and it is suggested you start with it."
      ]
    },
    {
      "metadata": {
        "id": "pypyhLsXUk0_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "1412584d-203b-4ec2-df6d-4efb91c0ab9d"
      },
      "cell_type": "code",
      "source": [
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uY8v2ohtciKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are 6 possible actions and 500 possible states. The actions are to go south, west, north, east, pick up the passenger, and drop off the passenger. The states are the 25 locations on the grid times the possible passenger locations (5: each of the destinations plus in the taxi) times the number of destinations (4)."
      ]
    },
    {
      "metadata": {
        "id": "oXMdiwLMUs-0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialize Q-table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zr0FW2drVTrq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "f8065076-ee8c-4023-d1b3-7a1f14bfb3c7"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.125       # learning rate (0 < alpha <= 1) - how much Q-values update per iteration\n",
        "gamma = 0.65      # discount factor (0 <= gamma <= 1) - how much future rewards are discounted\n",
        "epsilon = 0.01      #\n",
        "\n",
        "# Plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "total_reward = 0\n",
        "rewards = []\n",
        "\n",
        "for i in range(1, 10001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        total_reward += reward\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episodes: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")\n",
        "print(\"Total rewards:\", total_reward)\n",
        "print(\"Average rewards:\", total_reward / i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episodes: 10000\n",
            "Training finished.\n",
            "\n",
            "Total rewards: 80073\n",
            "Average rewards: 8.0073\n",
            "CPU times: user 3.47 s, sys: 302 ms, total: 3.77 s\n",
            "Wall time: 3.64 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yrFEQtwUf8xp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f32098c4-ad54-4b4b-80a4-34bfda920ef4"
      },
      "cell_type": "code",
      "source": [
        "q_table[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "       [-1.63022799e+00, -9.49568408e-01, -1.62440781e+00,\n",
              "        -9.74550511e-01,  8.41938772e-02, -9.96389618e+00],\n",
              "       [ 2.05885936e-02,  1.60520479e+00,  2.03995004e-03,\n",
              "         1.62358665e+00,  4.10460089e+00, -7.43091036e+00],\n",
              "       [-2.05068772e+00, -1.61947154e+00, -2.05493319e+00,\n",
              "        -1.61744124e+00, -9.45273980e-01, -1.06321542e+01],\n",
              "       [-2.63528417e+00, -2.65279519e+00, -2.63528489e+00,\n",
              "        -2.63537936e+00, -1.10424937e+01, -1.06412247e+01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "hUOmwSDYC9Xw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Goal 2 - Beat Basic Q-learning**\n",
        "Once you've got an initial Q-learning approach working, you should try to improve it via hyperparameter optimization. A score (average performance across many games) generated without optimizing hyperparameters that you should try to beat: 8.467\n",
        "\n",
        "You should be able to do better without having to use different techniques (i.e. just with hyperparameter optimization)."
      ]
    },
    {
      "metadata": {
        "id": "Lcx_1u-8DCw1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The following is from Moustafa Alzantot's article on Genetic algorithms for \n",
        "# hyperparameter tuning\n",
        "\n",
        "# I was unable to tweak it sufficiently in time to apply to this sprint challenge\n",
        "\n",
        "# -----------------------------------------------\n",
        "# def run_episode(env, policy, episode_len=100):\n",
        "#     total_reward = 0\n",
        "#     obs = env.reset()\n",
        "#     for t in range(episode_len):\n",
        "#         # env.render()\n",
        "#         action = policy[obs]\n",
        "#         obs, reward, done, _ = env.step(action)\n",
        "#         total_reward += reward\n",
        "#         if done:\n",
        "#             # print('Epside finished after {} timesteps.'.format(t+1))\n",
        "#             break\n",
        "#     return total_reward\n",
        "\n",
        "\n",
        "# def evaluate_policy(env, policy, n_episodes=100):\n",
        "#     total_rewards = 0.0\n",
        "#     for _ in range(n_episodes):\n",
        "#         total_rewards += run_episode(env, policy)\n",
        "#     return total_rewards / n_episodes\n",
        "\n",
        "# def gen_random_policy():\n",
        "#     return np.random.choice(6, size=((500)))\n",
        "\n",
        "# def crossover(policy1, policy2):\n",
        "#     new_policy = policy1.copy()\n",
        "#     for i in range(16):\n",
        "#         rand = np.random.uniform()\n",
        "#         if rand > 0.5:\n",
        "#             new_policy[i] = policy2[i]\n",
        "#     return new_policy\n",
        "\n",
        "# def mutation(policy, p=0.05):\n",
        "#     new_policy = policy.copy()\n",
        "#     for i in range(16):\n",
        "#         rand = np.random.uniform()\n",
        "#         if rand < p:\n",
        "#             new_policy[i] = np.random.choice(4)\n",
        "#     return new_policy\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     random.seed(1234)\n",
        "#     np.random.seed(1234)\n",
        "#     env = gym.make('Taxi-v2')\n",
        "#     env.seed(0)\n",
        "#     ## Policy search\n",
        "#     n_policy = 100\n",
        "#     n_steps = 20\n",
        "#     start = time.time()\n",
        "#     policy_pop = [gen_random_policy() for _ in range(n_policy)]\n",
        "#     for idx in range(n_steps):\n",
        "#         policy_scores = [evaluate_policy(env, p) for p in policy_pop]\n",
        "#         print('Generation %d : max score = %0.2f' %(idx+1, max(policy_scores)))\n",
        "#         policy_ranks = list(reversed(np.argsort(policy_scores)))\n",
        "#         elite_set = [policy_pop[x] for x in policy_ranks[:5]]\n",
        "#         select_probs = np.array(policy_scores) / np.sum(policy_scores)\n",
        "#         child_set = [crossover(\n",
        "#             policy_pop[np.random.choice(range(n_policy), p=select_probs)], \n",
        "#             policy_pop[np.random.choice(range(n_policy), p=select_probs)])\n",
        "#             for _ in range(n_policy - 5)]\n",
        "#         mutated_list = [mutation(p) for p in child_set]\n",
        "#         policy_pop = elite_set\n",
        "#         policy_pop += mutated_list\n",
        "#     policy_score = [evaluate_policy(env, p) for p in policy_pop]\n",
        "#     best_policy = policy_pop[np.argmax(policy_score)]\n",
        "\n",
        "#     end = time.time()\n",
        "#     print('Best policy score = %0.2f. Time taken = %4.4f'\n",
        "#             %(np.max(policy_score), (end-start)))    \n",
        "\n",
        "#     ## Evaluation\n",
        "#     env = wrappers.Monitor(env, '/tmp/frozenlake1', force=True)\n",
        "#     for _ in range(200):\n",
        "#         run_episode(env, best_policy)\n",
        "#     env.close()\n",
        "# gym.upload('/tmp/frozenlake1', api_key=...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_LLnLrLdDDo-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Goal 3 - Beat Optimized Q-learning (stretch)**\n",
        "Now the sky's the limit - or rather, the best possible performance in Taxi. With the default environment, optimized Q-learning has achieved an average score of 9.423. See if you can get in that range, or possibly even beat it, by employing alternative techniques.\n",
        "\n",
        "What is an alternative technique? It's anything that maps from environment state to action - Q-learning achieves this by populating a Q-table, but any model that can take the environment state and possible action as input and give predicted reward as output can serve the same purpose. And the Gym environment object gives us a simulator perfect for generating arbitrary amounts of training data to train such a model.\n",
        "\n",
        "The true optimal performance in Taxi is probably not much more than the optimized Q-learning score - the score certainly has to be less than 20 as the taxi must always take at least some steps to achieve the task.\n",
        "\n",
        "If you get this far, feel free to share the best score you get, and see how your classmates are doing. Good luck!"
      ]
    },
    {
      "metadata": {
        "id": "cS9-IIj_DIgP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}